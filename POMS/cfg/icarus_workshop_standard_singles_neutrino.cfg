# test config for gen in ICARUS experiment
[global]
#
# This section has variables we use later in the file as %(name)s
#
group      = icarus
experiment = icarus
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
# override on command line with proper sw. version
version = v08_19_01 
# override on command line with proper qualifiers
quals   = e17:prof
# overridden in --stage sections (below)
basename = gen
# usually overridden on command line with poms
sam_dataset = override_me
fclfile = standard_%(basename)s_%(experiment)s.fcl
sample = numu_bnb
campaign = MCC1.1
prodstatus = production
prodname = %(experiment)s_prod_%(sample)s_%(version)s 
#Number of events per job
nevt=-1
# overridden in --stage sections (below) or on cmd line
# for merge passes, override on command line
streamname = only

[env_pass]
#
# these become -e parameters to jobsub_submit
#
IFDH_DEBUG = 1
SAM_EXPERIMENT=%(experiment)s
SAM_GROUP=%(group)s
SAM_STATION=%(experiment)s

[submit]
#
# these become options to jobsub_submit
#
G          = %(group)s
#N          = 5000

#dataset     =
resource-provides      = usage_model=OPPORTUNISTIC,DEDICATED,OFFSITE
generate-email-summary = True
expected-lifetime      = 4h
#timeout                = 2h
OS                     = SL6
#disk                  = 10GB
memory                 = 1000MB
n_files_per_job        = 1
append_condor_requirements = ((TARGET.has_avx==true)&&(TARGET.HAS_CVMFS_icarus_opensciencegrid_org==true))
line_1 = +PeriodicRemove=JobStatus==5&&HoldReasonCode==26&&CurrentTime-EnteredCurrentStatus>3600

[job_setup]
#
# these are options to fife_wrap about setting up the job environment,
# and main execution loop
#
debug        = True
find_setups  = True
source_1     = /cvmfs/%(experiment)s.opensciencegrid.org/products/%(experiment)s/setup_%(experiment)s.sh
setup_1      = %(experiment)scode %(version)s -q %(quals)s
multifile    = True
#getconfig   = False
#ifdh_art    = False

[sam_consumer]
#
# parameters to SAM / ifdh establishProcess
#
limit       = 1
appvers     = %(version)s
appfamily   = art
appname     = %(basename)s
#schema     = gsiftp
schema      = root

[executable]
#
# parameters to main executable in job
#
name       = lar
arg_1      = -c
arg_2      = %(fclfile)s
arg_3      = -o
arg_4      = %(basename)s.root
arg_5      = -T 
arg_6      = hist_%(basename)s.root
arg_7      = -n 
arg_8      = %(nevt)s 
arg_9      = -s 
#arg_10      = input_filename -- will be added by multifile loop...

[job_output]
#
# parameters to output handling section of fife_wrap
#
addoutput   = %(basename)s.root
rename      = unique
dest        = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/poms_%(prodstatus)s/MCC1_poms_%(prodname)s/%(basename)s/
declare_metadata=True
metadata_extractor=sam_metadata_dumper
add_location=True          

[job_output_1]
#
# parameters to output handling section of fife_wrap
#
addoutput   = histo_%(basename)s.root
rename      = unique
dest        = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/poms_histo/
declare_metadata = True
metadata_extractor=histo_metadata_dumper --input %%s --foo --bar

[job_output_2]
#
# parameters to output handling section of fife_wrap
#
addoutput   = *.[ol][ou][gt]
dest        = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/logs/
#
# now we have overides for each processing stage/job type 
#
# you may need to change the global.fclfile overrides for each
# stage to reflect your experiment's naming convention for .fcl files
#
[stage_gen]
# fake output dataset for POMS
job_output.add_to_dataset = _poms_task
job_output.dataset_exclude = hist*
# turn off -s flag
executable.arg_7  = 
global.basename   = gen
global.fclfile    = standard_genie_%(experiment)s.fcl
#submit.dataset   = %(sam_dataset)s
# set job_setup.multifile to false because we don't have input for gen stage. 
# If set to true, POMS will try to call the SAM PROJECT for non-existent input files
# and causing the job to fail with user exit code 1 -Maya
job_setup.multifile = False 

[stage_filter]
global.basename   = filter
submit.dataset    = %(sam_dataset)s
global.fclfile    = %(basename)s_genie_active.fcl
submit.expected-lifetime = 2h
submit.memory     = 300MB
job_output.dest   = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/poms_%(prodstatus)s/MCC1_poms_%(prodname)s/%(basename)s/

[stage_g4]
global.basename   = g4
submit.dataset    = %(sam_dataset)s
global.fclfile    = standard_%(basename)s_%(experiment)s.fcl
submit.expected-lifetime = 4h
submit.memory     = 3000MB
#job_output.dest   = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/test/origin/MCC1_poms_%(prodname)s/%(basename)s/
job_output.dest   = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/poms_%(prodstatus)s/MCC1_poms_%(prodname)s/%(basename)s/
# # if g4 only works onsite 
# submit.resource-provides= usage_model=OPPORTUNISTIC,DEDICATED
#
# # ...with extra cvmfs libraries:
# job_setup.prescript     = export LD_LIBRARY_PATH=/cvmfs/nova.opensciencegrid.org/externals/library_shim/v03.03/NULL/lib/sl6:$LD_LIBRARY_PATH

[stage_detsim]
global.basename   = detsim
submit.dataset    = %(sam_dataset)s
global.fclfile    = standard_%(basename)s_%(experiment)s.fcl
submit.expected-lifetime = 4h
submit.memory     = 3000MB
#job_output.dest   = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/test/origin/MCC1_poms_%(prodname)s/%(basename)s/
job_output.dest   = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/poms_%(prodstatus)s/MCC1_poms_%(prodname)s/%(basename)s/

[stage_reco]
global.basename   = reco
submit.dataset    = %(sam_dataset)s
global.fclfile    = %(basename)s_%(experiment)s_driver_%(basename)s_all.fcl
submit.expected-lifetime = 4h
submit.memory     = 3500MB
#job_output.dest   = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/test/origin/MCC1_poms_%(prodname)s/%(basename)s/
job_output.dest   = /pnfs/%(experiment)s/scratch/users/%(experiment)spro/dropbox/mc1/poms_%(prodstatus)s/MCC1_poms_%(prodname)s/%(basename)s/

[stage_anatree]
global.basename = anatree
submit.dataset  = %(sam_dataset)s

[stage_split]
global.basename = split
arg_3           =
arg_4           =
submit.dataset  = %(sam_dataset)s

[stage_merge]
global.basename = merge_%(streamname)s
submit.dataset  = %(sam_dataset)s
